{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10288dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from statistics import mean\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,label_binarize, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer,roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Lasso,LinearRegression, LassoCV\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import shap\n",
    "import warnings\n",
    "from sklearn.feature_selection import SelectKBest, RFE\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from PyFingerprint.fingerprint import get_fingerprint, get_fingerprints\n",
    "# Suppress RDKit deprecation warnings\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fingerprints(df, smiles_col, fp_type):\n",
    "    \"\"\"\n",
    "    Calculates fingerprints for a given type and add them to the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame with a SMILES column.\n",
    "    - smiles_col (str): Name of the column containing SMILES strings.\n",
    "    - fp_type (str): The fingerprint type to calculate (from predefined lists).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The original DataFrame with fingerprint columns added.\n",
    "    \"\"\"\n",
    "    # Checking if the fingerprint type is valid\n",
    "    valid_types = ['standard', 'extended', 'graph', 'maccs', 'pubchem', 'estate', \n",
    "                   'hybridization', 'lingo', 'klekota-roth', 'shortestpath', \n",
    "                   'cdk-substructure', 'rdkit', 'morgan', 'rdk-maccs', \n",
    "                   'topological-torsion', 'avalon', 'atom-pair', 'mol2vec']\n",
    "    \n",
    "    if fp_type not in valid_types:\n",
    "        raise ValueError(f\"Invalid fingerprint type '{fp_type}'. Choose from {valid_types}.\")\n",
    "    \n",
    "    # a helper function for fingerprint calculation\n",
    "    def calculate_fp(smi):\n",
    "        try:\n",
    "            fp = get_fingerprint(smi, fp_type)\n",
    "            # Convert to list for easier addition to DataFrame\n",
    "            return fp.to_numpy().tolist() if hasattr(fp, 'to_numpy') else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating fingerprint for SMILES '{smi}': {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Calculates fingerprints and store as a new column\n",
    "    fingerprints = df[smiles_col].apply(calculate_fp)\n",
    "    \n",
    "    # Splits fingerprint lists into separate columns\n",
    "    fingerprint_df = pd.DataFrame(fingerprints.tolist(), index=df.index)\n",
    "    fingerprint_df.columns = [f\"{fp_type}_fp_{i}\" for i in range(fingerprint_df.shape[1])]\n",
    "    \n",
    "    return_df = pd.concat([df, fingerprint_df], axis=1)\n",
    "    ic50_column = return_df.pop('IC50')  \n",
    "    return_df['IC50'] = ic50_column         \n",
    "\n",
    "    # Removes the 'SMILES' column\n",
    "    return_df = return_df.drop(columns=['SMILES'])\n",
    "    # Combines with the original DataFrame\n",
    "    return return_df\n",
    "\n",
    "def model_pipeline(df_smiles, smiles_col, perform_feature_selection=False, perform_pca=False):\n",
    "    \"\"\"\n",
    "    Performs the full model development and evaluation pipeline for all fingerprints.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_smiles (pd.DataFrame): The DataFrame containing the SMILES strings. (data resulting from data preparation process in \n",
    "    previous code file)\n",
    "    - smiles_col (str): The column containing SMILES strings.\n",
    "    - perform_feature_selection (bool): Whether to perform feature selection using Lasso. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function prints the evaluation results for each fingerprint and model.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    feature_flag = False\n",
    "    pca_flag = False\n",
    "    # List of fingerprint types\n",
    "    fingerprint_types = ['standard', 'extended', 'graph', 'maccs', 'pubchem', 'estate', \n",
    "                   'hybridization', 'lingo', 'klekota-roth', 'shortestpath', \n",
    "                   'cdk-substructure', 'rdkit', 'morgan', 'rdk-maccs', \n",
    "                   'topological-torsion', 'avalon', 'atom-pair', 'mol2vec']\n",
    "\n",
    "    # Defines parameter grids for each model\n",
    "    param_grids = {\n",
    "        \"RandomForest\": {\n",
    "             \"n_estimators\": [150]\n",
    "        },\n",
    "         \"DecisionTree\": {\n",
    "             \"max_depth\": [5, 10, 15, 20, None],\n",
    "             \"min_samples_split\": [2, 5, 10],\n",
    "             \"min_samples_leaf\": [1, 2, 4],\n",
    "             \"max_features\": ['sqrt', 'log2', None]\n",
    "         },\n",
    "         \"SVM\": {\n",
    "             \"C\": [0.1, 1, 10, 100],\n",
    "             \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "             \"gamma\": [\"scale\", \"auto\"]\n",
    "         },\n",
    "         \"XGBoost\": {\n",
    "             \"n_estimators\": [150]\n",
    "         },\n",
    "         \"KNN\": {\n",
    "             \"n_neighbors\": [3, 5, 7, 10],\n",
    "             \"weights\": ['uniform', 'distance'],\n",
    "             \"metric\": ['euclidean', 'manhattan', 'chebyshev']\n",
    "         }, \n",
    "         \"MLP\": {\n",
    "         \"hidden_layer_sizes\": [(100, 100)],\n",
    "         \"activation\": [\"tanh\"],\n",
    "         \"solver\": [\"sgd\"],\n",
    "         \"alpha\": [0.0001],\n",
    "         \"learning_rate\": [\"adaptive\"]\n",
    "         }\n",
    "    }\n",
    "\n",
    "    # Defines models for each algorithm\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"SVM\": SVC(probability=True, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "        \"KNN\": KNeighborsClassifier(), \n",
    "        \"MLP\": MLPClassifier(random_state=42, max_iter=10000)\n",
    "    }\n",
    "\n",
    "    # Function to calculate and print metrics\n",
    "    def evaluate_and_print_results(model_name, y_test, y_pred, y_probs, fing_type, feature_selection, pca, results, hyperparams=None):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "        \n",
    "        # Compute ROC and AUC\n",
    "        y_test_bin = label_binarize(y_test, classes=[0, 1]).ravel()\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin, y_probs)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        # Append results to the list\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"fingerprint type\": fing_type,\n",
    "            \"Best Parameters\": hyperparams,\n",
    "            \"feature_selection\": feature_selection,\n",
    "            \"PCA\": pca,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Sensitivity (Recall)\": sensitivity,\n",
    "            \"Specificity\": specificity,\n",
    "            \"Precision\": precision,\n",
    "            \"AUC score\": auc_score, \n",
    "            \"Confusion Matrix\": f\"TP={tp}, TN={tn}, FP={fp}, FN={fn}\"\n",
    "        })\n",
    "\n",
    "        print(f\"Results for {model_name}\")\n",
    "        if hyperparams:\n",
    "            print(f\"Hyperparameters: {hyperparams}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"AUC: {auc_score: .4f}\")\n",
    "        print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    # Loop through all fingerprints\n",
    "    for fp_type in fingerprint_types:\n",
    "        print(f\"\\nProcessing fingerprint: {fp_type}\")\n",
    "        \n",
    "        # Step 1: Extract fingerprints for the current type\n",
    "        updated_df = calculate_fingerprints(df_smiles, smiles_col, fp_type)\n",
    "        \n",
    "        # Checking for missing values\n",
    "        print(\"Missing values: \", updated_df.isna().all().sum())\n",
    "        print(updated_df['IC50'].value_counts())\n",
    "        \n",
    "\n",
    "        X = updated_df.iloc[:, :-1]\n",
    "        y = updated_df.iloc[:, -1]\n",
    "        \n",
    "\n",
    "        # Initialize the LabelEncoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        updated_df[updated_df.columns[-1]] = y\n",
    "        \n",
    "\n",
    "        # Step 2: Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale the data\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "\n",
    "        # Performing Lasso feature selection if requested\n",
    "        if perform_feature_selection:\n",
    "            feature_flag = True\n",
    "            print(\"Performing feature selection with Lasso...\")\n",
    "            lasso = LassoCV(cv=5, random_state=42, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "            selected_features_mask = lasso.coef_ != 0\n",
    "            selected_feature_names = X_train.columns[selected_features_mask]\n",
    "            print(\"Number of selected features:\", len(selected_feature_names))\n",
    "            print(\"Selected features:\", list(selected_feature_names))\n",
    "\n",
    "            X_train_scaled = X_train_scaled[:, selected_features_mask]\n",
    "            X_test_scaled = X_test_scaled[:, selected_features_mask]\n",
    "            \n",
    "        if perform_pca:\n",
    "            pca_flag = True\n",
    "            # Step 2: Perform PCA on the training set\n",
    "            pca = PCA()\n",
    "            pca.fit(X_train_scaled)\n",
    "\n",
    "            # Step 3: Compute the cumulative explained variance ratio\n",
    "            explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "            # Step 4: Determine the number of principal components for 95% variance\n",
    "            num_components = np.argmax(explained_variance_ratio >= 0.99) + 1\n",
    "\n",
    "            # Step 5: Project training and test data onto selected components\n",
    "            pca = PCA(n_components=num_components)\n",
    "            X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "            X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "        # Step 3: Model training and evaluation\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nTraining {model_name} model\")\n",
    "\n",
    "            if model_name == \"SVM\":\n",
    "                # Train SVM with fixed hyperparameters\n",
    "                print(\"Using fixed hyperparameters for SVM...\")\n",
    "                fixed_params = {\"kernel\": \"rbf\"}\n",
    "                model.set_params(**fixed_params)\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                results = evaluate_and_print_results(model_name, y_test, y_pred, y_probs, fp_type, feature_flag, pca_flag, results, fixed_params)\n",
    "            else:\n",
    "                # Set up GridSearchCV for the current model\n",
    "                grid_search = GridSearchCV(model, param_grid=param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "                grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "                # Get the best model and best parameters from GridSearchCV\n",
    "                best_model = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "\n",
    "                # Evaluate the model on the test set\n",
    "                y_pred = best_model.predict(X_test_scaled)\n",
    "                y_probs = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "                results = evaluate_and_print_results(model_name, y_test, y_pred, y_probs, fp_type, feature_flag, pca_flag, results, best_params)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    if perform_feature_selection:\n",
    "        results_df.to_excel(\"results_features.xlsx\", index=False)\n",
    "    elif perform_pca:\n",
    "        results_df.to_excel(\"results_pca.xlsx\", index=False)\n",
    "    else:\n",
    "        results_df.to_excel(\"results_80.xlsx\", index=False)\n",
    "    feature_flag = False\n",
    "    pca_flag = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_env)",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
